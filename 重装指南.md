第一步：

将桌面等改到非系统盘，设定好软件盘和文件盘

第二步：

压缩工具7z

第三步：

软件：

1. 百度云 clash PotPlayer ,listary

2. 向日葵

​	远程：213 467 360    f4rf20

3. 交流工具 qq \微信\tg\twitter\腾讯会议  设置下载保存目录

4. 火绒

5. typora/sublime
6. git/github
7. 语言，java1.8/pyhon3.9/scala2.12.11/mysql5.6
8. ide, idea/pycharm/vscode/vm/finalshell/datagrip



git安装并连到远程仓库：
官网下载安装之后，打开git bush

```shell
ssh-keygen -t rsa -C 你注册github时的邮箱地址
```

C:\Users\Tal\.ssh路径下*打开id_rsa.pub* 并复制内容

github-setting-ssh and gpg keys-new ssh key粘贴

测试ssh key

```shell
ssh -T git@github.com
git config --global user.name 你github的名字 
git config --global user.email 你的github邮箱地址
```





java1.8 安装配置

安装好后，系统变量里添加JAVA_HOME，内容为jdk路径

创建变量“**CLASSPATH**”，变量名为“.;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar;”

找到**Path**变量，在里面添加一行“;%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin”

cmd测试 java -version 



python直接安装就行，勾选add to path

cmd测试 python



mysql5.7.41下包解压，然后目录下建data文件夹和my.ini文件

```ini
[mysqld]
#端口号
port = 3306
#数据库的安装路径
basedir=D:\mysql\mysql-5.7.29-winx64
#数据库的安装路径+\data(数据库存储路径)
datadir=D:\mysql\mysql-5.7.41-winx64\data
#最大连接数
max_connections=200
#编码
character-set-server=utf8
default-storage-engine=INNODB
sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES
[mysql]
#编码
default-character-set=utf8
```

```shell
mysqld -install

mysqld --initialize-insecure --user=mysql

net start mysql

mysql -u root -p
```

```mysql
use mysql

update user set authentication_string=password("1234") where user="root"
```







idea、pycharm安装后bin/idea.properties修改存储路径

maven和pip存储路径也需要修改

下载安装VM 17 Pro，搜索破解

JU090-6039P-08409-8J0QH-2YR7F或者 MC60H-DWHD5-H80U9-6V85M-8280D

下载CentOS 7安装，

安装jdk-8u65-linux-x64.tar.gz、hadoop-3.3.5.tar.gz、spark-3.4.0-bin-hadoop3.tgz

```shell
tar -zxvf /data/packages/jdk-8u65-linux-x64.tar.gz -C /software/		#解压jdk
tar -zxvf /data/packages/hadoop-3.3.5.tar.gz -C /software/    		#解压hadoop
tar -zxvf /data/packages/spark-3.4.0-bin-hadoop3.tgz -C /software/   	 # 解压Spark
```

配置环境变量

```shell
# 改名
mv /software/jdk1.8.0_65/ /software/java
mv /software/hadoop-3.3.5/ /software/hadoop
mv /software/spark-3.4.0-bin-hadoop3/ /software/spark

chown -R root /software/spark
chgrp -R root /software/spark

vi 	/etc/profile   #修改环境变量
#写在文件末尾即可
# java 
export JAVA_HOME=/software/java

# hadoop
export HADOOP_HOME=/software/hadoop

# Spark
export SPARK_HOME=/software/spark

export PATH=$PATH:$HADOOP_HOME/bin:$JAVA_HOME/bin:$HADOOP_HOME/sbin:$JAVA_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOMR/sbin


. /etc/profile   #刷新配置文件
```

```shell
java -version
hadoop version
spark-shell
#检查环境变量是否出错
```

Hadoop 相关配置

```shell
cd /software/hadoop/etc/hadoop/
# 先打开配置文件所在目录方便后续操作

# 修改 hadoop-env.sh 
vi hadoop-env.sh 
#配置JAVA_HOME
export JAVA_HOME=/software/java
#设置用户以执行对应角色shell命令
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root 

```

```xml
# --修改 core-site.xml   写在configration标签里，后同
vi core-site.xml


<!-- 默认文件系统的名称。通过URI中schema区分不同文件系统。-->
<!-- file:///本地文件系统 hdfs:// hadoop分布式文件系统 gfs://。-->
<!-- hdfs文件系统访问地址：http://nn_host:9000。-->
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://master:9000</value>
</property>
<!-- hadoop本地数据存储目录 format时自动生成 -->
<property>
    <name>hadoop.tmp.dir</name>
    <value>/data/hadoop</value>
</property>
<!-- 在Web UI访问HDFS使用的用户名。-->
<property>
    <name>hadoop.http.staticuser.user</name>
    <value>root</value>
</property>


hdfs-site.xml
vi hdfs-site.xml
<!-- 设定SNN运行主机和端口。-->
<property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>slave1:9868</value>
</property>

mapred-site.xml
vi mapred-site.xml
<!-- mr程序默认运行方式。yarn集群模式 local本地模式-->
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>
<!-- hadoop3特有的配置 -->
<!-- MR App Master环境变量。-->
<property>
  <name>yarn.app.mapreduce.am.env</name>
  <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
<!-- MR MapTask环境变量。-->
<property>
  <name>mapreduce.map.env</name>
  <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
<!-- MR ReduceTask环境变量。-->
<property>
  <name>mapreduce.reduce.env</name>
  <value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>

yarn-site.xml
vi yarn-site.xml
<!-- yarn集群主角色RM运行机器。-->
<property>
    <name>yarn.resourcemanager.hostname</name>
    <value>master</value>
</property>
<!-- NodeManager上运行的附属服务。需配置成mapreduce_shuffle,才可运行MR程序。-->
<property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
</property>
<!-- 每个容器请求的最小内存资源（以MB为单位）。-->
<property>
  <name>yarn.scheduler.minimum-allocation-mb</name>
  <value>512</value>
</property>
<!-- 每个容器请求的最大内存资源（以MB为单位）。-->
<property>
  <name>yarn.scheduler.maximum-allocation-mb</name>
  <value>2048</value>
</property>
<!-- 容器虚拟内存与物理内存之间的比率。-->
<property>
  <name>yarn.nodemanager.vmem-pmem-ratio</name>
  <value>4</value>
</property>

```

```shell
vi workers

master
slave1
slave2
```

spark standalone

```shell
# 在cd /software/spark/conf下 
mv workers.template slaves
vi slaves

slave1
slave2

mv spark-env.sh.template  spark-env.sh
vi spark-env.sh 

## 设置JAVA安装目录
JAVA_HOME=/software/java

## HADOOP软件配置文件目录，读取HDFS上文件和运行Spark在YARN集群时需要,先提前配上
HADOOP_CONF_DIR=/software/hadoop/etc/hadoop
YARN_CONF_DIR=/software/hadoop/etc/hadoop

## 指定spark老大Master的IP和提交任务的通信端口
SPARK_MASTER_HOST=master
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8080
SPARK_WORKER_CORES=1
SPARK_WORKER_MEMORY=1g

```

克隆虚拟机，克隆前需关闭虚拟机,克隆两个（完整克隆）

```shell
vi /etc/hostname  #修改主机名 分别改为slave1 和slave2 （和前面预先配置的保持一致）
vi /etc/sysconfig/network-scripts/ifcfg-ens33  #修改 IP地址，和前面的network里和hosts里的保持一致
```

```shell
ssh-keygen  #ssh密钥生成,连续4次回车
# node1上分步执行，每次都需要输入密码
ssh-copy-id master
ssh-copy-id slave1
ssh-copy-id slave2

hdfs namenode -format   #在node01 上执行
start-all.sh #测试

/software/spark/sbin/start-all.sh #测试spark standalone
```

修改本地host文件，不然不能主机名访问

http://master:8080/   

OnYarn

关闭之前的Spark-Standalone集群

```shell
 /software/spark/sbin/stop-all.sh
```

1.配置Yarn历史服务器并关闭资源检查

```shell
vi /software/hadoop/etc/hadoop/yarn-site.xml
```

```xml
<configuration>
    <!-- 配置yarn主节点的位置 -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <!-- 设置yarn集群的内存分配方案 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>20480</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>2048</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>2.1</value>
    </property>
    <!-- 开启日志聚合功能 -->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <!-- 设置聚合日志在hdfs上的保存时间 -->
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>
    <!-- 设置yarn历史服务器地址 -->
    <property>
        <name>yarn.log.server.url</name>
        <value>http://master:19888/jobhistory/logs</value>
    </property>
    <!-- 关闭yarn内存检查 -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
</configuration>
```

复制配置到其他节点

```shell
cd /software/hadoop/etc/hadoop
scp -r yarn-site.xml root@slave1:$PWD
scp -r yarn-site.xml root@slave2:$PWD
```

2.配置Spark的历史服务器和Yarn的整合

```shell
cd /software/spark/conf

mv spark-defaults.conf.template spark-defaults.conf
vi spark-defaults.conf
# 添加以下内容
spark.eventLog.enabled                  true
spark.eventLog.dir                      hdfs://master:9000/sparklog/
spark.eventLog.compress                 true
spark.yarn.historyServer.address        master:18080

vi /software/spark/conf/spark-env.sh
# 添加以下内容
## 配置spark历史日志存储地址
SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://master:9000/sparklog/ -Dspark.history.fs.cleaner.enabled=true"

# sparklog需要手动创建
hadoop fs -mkdir -p /sparklog
# （创建目录失败，可能是安全模式导致的，关闭安全模式即可）
hadoop  dfsadmin -safemode leave

# 修改日志级别
cd /software/spark/conf
mv log4j2.properties.template log4j2.properties

# 分发
cd /software/spark/conf
scp -r spark-env.sh root@slave1:$PWD
scp -r spark-env.sh root@slave2:$PWD

scp -r spark-defaults.conf root@slave1:$PWD
scp -r spark-defaults.conf root@slave2:$PWD

scp -r log4j2.properties root@slave1:$PWD
scp -r log4j2.properties root@slave2:$PWD

```

3.配置依赖的Spark 的jar包

```shell
# 1. 在HDFS上创建存储spark相关jar包的目录
hadoop fs -mkdir -p /spark/jars/

# 2. 上传$SPARK_HOME/jars所有jar包到HDFS
hadoop fs -put /software/spark/jars/* /spark/jars/

# 3. 在master上修改spark-defaults.conf
vi /software/spark/conf/spark-defaults.conf
# 添加
spark.yarn.jars  hdfs://master:9000/spark/jars/*
# 分发
cd /software/spark/conf
scp -r spark-defaults.conf root@slave1:$PWD
scp -r spark-defaults.conf root@slave2:$PWD
```

4.启动服务

```shell
# 在master上启动Hadoop集群
start-all.sh
# 在master上启动MRHistoryServer服务
mr-jobhistory-daemon.sh start historyserver
# 在master上启动SparkHistoryServer服务
/software/spark/sbin/start-history-server.sh
```

\- MRHistoryServer服务WEB UI页面：

http://master:19888



安装windows的scala2.12.11 、hadoop-3.3.5、spark-3.4.0-bin-hadoop3
